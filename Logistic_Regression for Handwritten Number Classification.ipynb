{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMVuUyGzG5ez"
      },
      "source": [
        "## Logistic Regression for Handwritten Digits Classfication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUnwBd9HnkrV"
      },
      "source": [
        "\n",
        "### Handwritten Digits Classification using Logistic Regression with Gradient Ascent Algorithm\n",
        "\n",
        "Difference between Gradient Descent and Gradient Ascent\n",
        "\n",
        "To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point.\n",
        "\n",
        "If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.\n",
        "\n",
        "The equation to update the logistic regression model parameters are:    \n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta^{+} = \\theta^{-} + \\alpha (y_{i} - h(x_{i}) )\\bar{x}\n",
        "\\end{equation}\n",
        "\n",
        "This maximizes the following log likelihood function\n",
        "\n",
        "\\begin{equation}\n",
        "J(x, \\theta, y) = \\sum_{i=1}^{m}y_i\\log(h(x_{i})) + (1 - y_i)\\log(1 - h(x_{i}))\n",
        "\\end{equation}\n",
        "\n",
        "where our hypothesis is a sigmoid function\n",
        "\\begin{equation}\n",
        "h(x_i) = \\frac{1}{1 + e^{\\theta^T \\bar{x}}}\n",
        "\\end{equation}\n",
        "\n",
        "### Batch gradient Ascent\n",
        "```FOR j FROM 0 -> max_iteration: \n",
        "    FOR i FROM 0 -> m: \n",
        "        theta += (alpha) * (y[i] - h(x[i])) * x_bar\n",
        "    ENDLOOP\n",
        "ENDLOOP\n",
        "```\n",
        "\n",
        "### Multi-class Classificaton with one-vs-all (one-vs-rest)\n",
        "If you have n-classes, we train n-classifiers and given a new data point we predict using all the classifiers and choose the one with the highest probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YppAr1nZQYT"
      },
      "source": [
        "# Do not change anything in this cell.\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiDnN8cApVoN"
      },
      "source": [
        "We already have the dataset ready for you. Here is a look at your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LbsU8qEZoxI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "b29181a7-69d9-4c46-b721-c5cd46e5c89a"
      },
      "source": [
        "# Do not change anything in this cell.\n",
        "digits = load_digits()\n",
        "\n",
        "f, axarr = plt.subplots(3,3)\n",
        "axarr[0,0].imshow(digits.images[0])\n",
        "axarr[0,1].imshow(digits.images[1])\n",
        "axarr[0,2].imshow(digits.images[2])\n",
        "axarr[1,0].imshow(digits.images[3])\n",
        "axarr[1,1].imshow(digits.images[4])\n",
        "axarr[1,2].imshow(digits.images[5])\n",
        "axarr[2,0].imshow(digits.images[6])\n",
        "axarr[2,1].imshow(digits.images[7])\n",
        "axarr[2,2].imshow(digits.images[8])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJv-OeZmdRZT"
      },
      "source": [
        "# Write code in this cell to build your Logistic Regression Model\n",
        "class LogisticRegression():\n",
        "    \"\"\"Class for training and using a model for logistic regression\"\"\"\n",
        "    \n",
        "    def set_values(self, initial_params, alpha=0.01, max_iter=5000, class_of_interest=0):\n",
        "        \"\"\"Set the values for initial params, step size, maximum iteration, and class of interest\"\"\"\n",
        "        self.params = initial_params\n",
        "        self.alpha = alpha\n",
        "        self.max_iter = max_iter\n",
        "        self.class_of_interest = class_of_interest\n",
        "    \n",
        "    @staticmethod\n",
        "    def _sigmoid(x):\n",
        "        \"\"\"Sigmoide function\"\"\"  \n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "    \n",
        "    def predict(self, x_bar, params):\n",
        "        \"\"\"predict the probability of a class\"\"\"  \n",
        "                \n",
        "        return self._sigmoid(np.dot(params, x_bar))\n",
        "    \n",
        "    def _compute_cost(self, input_var, output_var, params):\n",
        "        \"\"\"Compute the log likelihood cost\"\"\"\n",
        "        \n",
        "        cost = 0  # Initialize the cost\n",
        "        input_var_with_1 = np.append(input_var, np.ones((input_var.shape[0], 1)), axis=1)\n",
        "        # Small constant, avoid log(0)\n",
        "        c = 0.0001\n",
        "        for x, y in zip(input_var_with_1, output_var):\n",
        "            #x_bar = np.append(x, 1)\n",
        "            x_bar = x\n",
        "            y_hat = y\n",
        "            h = self.predict(x_bar, self.params)\n",
        "            y_binary = int(y_hat == self.class_of_interest)  # Return 1 if \"y\" equals class of interest and 0 otherwise\n",
        "            cost += y_binary * np.log(h + c) + (1 - y_binary) * np.log(1 - h + c)\n",
        "\n",
        "        return cost\n",
        "    \n",
        "    def train(self, input_var, label, print_iter = 5000):\n",
        "        \"\"\"Train the model using batch gradient ascent\"\"\"\n",
        "        \n",
        "        iteration = 1\n",
        "        input_var_with_1 = np.append(input_var, np.ones((input_var.shape[0], 1)), axis=1)\n",
        "        while iteration < self.max_iter:\n",
        "            if iteration % print_iter == 0:\n",
        "                print(f'iteration: {iteration}')\n",
        "                print(f'cost: {self._compute_cost(input_var, label, self.params)}')\n",
        "                print('--------------------------------------------')\n",
        "\n",
        "            for i, xy in enumerate(zip(input_var_with_1, label)):\n",
        "                #x_bar = np.append(xy[0], 1)\n",
        "                x_bar = xy[0]\n",
        "                y_hat = xy[1]\n",
        "                h = self.predict(x_bar, self.params)\n",
        "                y_binary = int(y_hat == self.class_of_interest) # Return 1 if xy[1] equals class of interest and 0 otherwise\n",
        "                gradient = y_binary - h\n",
        "                self.params += self.alpha * gradient * x_bar\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        return self.params\n",
        "\n",
        "    def test(self, input_test, label_test):\n",
        "        \"\"\"Test the accuracy of the model using test data\"\"\"\n",
        "        # Note* You can write your own approach to test, this is just one approach\n",
        "        self.total_classifications = 0\n",
        "        self.correct_classifications = 0\n",
        "        input_test_with_1 = np.append(input_test, np.ones((input_test.shape[0], 1)), axis=1)\n",
        "        for x, y in zip(input_test_with_1, label_test):\n",
        "            self.total_classifications += 1\n",
        "            #x_bar = np.append(x, 1)\n",
        "            x_bar = x\n",
        "            y_hat = y\n",
        "            #h = self._sigmoid(np.dot(self.params, x_bar))\n",
        "            h = self.predict(x_bar, self.params)\n",
        "            y_binary = int(y_hat == self.class_of_interest)\n",
        "\n",
        "            # If predict is the class of interest and y is also class of interest\n",
        "            if h >= 0.5 and y_binary == 1:\n",
        "                # correct classification of class_of_interest\n",
        "                self.correct_classifications += 1\n",
        "\n",
        "            # If predict is not the class of interest and y is also not class of interest\n",
        "            if h < 0.5 and y_binary != 1:\n",
        "                # correct classification of an other class\n",
        "                self.correct_classifications += 1\n",
        "\n",
        "        self.accuracy = self.correct_classifications / self.total_classifications\n",
        "\n",
        "        return self.accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofrbV4Iv9fkj"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "digits_train, digits_test, digits_label_train, digits_label_test =\\\n",
        "train_test_split(digits.data, digits.target, test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzHbriAw9fzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f8910f-2e9a-499b-c247-2c74ce537302"
      },
      "source": [
        "# train a classifier for the ZERO digit\n",
        "alpha = 1e-2\n",
        "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
        "\n",
        "max_iter = 10000\n",
        "digits_regression_model_0 = LogisticRegression() # Initialize the class\n",
        "digits_regression_model_0.set_values(params_0, alpha, max_iter, 0) # set initial values\n",
        "\n",
        "params = digits_regression_model_0.train(digits_train / 16, digits_label_train, 1000) # Train the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 1000\n",
            "cost: -1.7297370086184407\n",
            "--------------------------------------------\n",
            "iteration: 2000\n",
            "cost: -0.9029669155935015\n",
            "--------------------------------------------\n",
            "iteration: 3000\n",
            "cost: -0.5900440081773833\n",
            "--------------------------------------------\n",
            "iteration: 4000\n",
            "cost: -0.42357168312754295\n",
            "--------------------------------------------\n",
            "iteration: 5000\n",
            "cost: -0.3197124413777849\n",
            "--------------------------------------------\n",
            "iteration: 6000\n",
            "cost: -0.2485348842185021\n",
            "--------------------------------------------\n",
            "iteration: 7000\n",
            "cost: -0.19661969580873206\n",
            "--------------------------------------------\n",
            "iteration: 8000\n",
            "cost: -0.15703291342916925\n",
            "--------------------------------------------\n",
            "iteration: 9000\n",
            "cost: -0.1258228795182585\n",
            "--------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkE0cAtF9f7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da25adc4-eb32-402f-8b32-943f0e7ec5f3"
      },
      "source": [
        "# accuracy\n",
        "digits_accuracy = digits_regression_model_0.test(digits_test / 16.0, digits_label_test)\n",
        "print(f'Accuracy of prediciting a ZERO digit in test set: {digits_accuracy}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of prediciting a ZERO digit in test set: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnZwvoD9-KPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fcb4ad3-bd24-4554-cdea-aa866c287254"
      },
      "source": [
        "# train a classifier for the ONE digit\n",
        "alpha = 1e-2\n",
        "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
        "\n",
        "max_iter = 10000\n",
        "digits_regression_model_1 = LogisticRegression() # Initialize the class\n",
        "digits_regression_model_1.set_values(params_0, alpha, max_iter, 1) # set initial values\n",
        "\n",
        "params = digits_regression_model_1.train(digits_train / 16, digits_label_train, 1000) # Train the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 1000\n",
            "cost: -34.77962747728378\n",
            "--------------------------------------------\n",
            "iteration: 2000\n",
            "cost: -28.124291754898763\n",
            "--------------------------------------------\n",
            "iteration: 3000\n",
            "cost: -24.62730505737384\n",
            "--------------------------------------------\n",
            "iteration: 4000\n",
            "cost: -22.32520031564121\n",
            "--------------------------------------------\n",
            "iteration: 5000\n",
            "cost: -20.638747737705177\n",
            "--------------------------------------------\n",
            "iteration: 6000\n",
            "cost: -19.320307648299348\n",
            "--------------------------------------------\n",
            "iteration: 7000\n",
            "cost: -18.243888508011278\n",
            "--------------------------------------------\n",
            "iteration: 8000\n",
            "cost: -17.337619832760875\n",
            "--------------------------------------------\n",
            "iteration: 9000\n",
            "cost: -16.55697277170131\n",
            "--------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdRONRR7-KYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfe08dd-de41-4ab5-d35d-8beafe4fa1fa"
      },
      "source": [
        "#accuracy\n",
        "digits_accuracy = digits_regression_model_1.test(digits_test / 16.0, digits_label_test)\n",
        "print(f'Accuracy of prediciting a ONE digit in test set: {digits_accuracy}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of prediciting a ONE digit in test set: 0.9805555555555555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baKdhDiI-Kqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed79df4-0d06-4383-9abf-db9b76a7e912"
      },
      "source": [
        "# train a classifier for the TWO digit\n",
        "alpha = 1e-2\n",
        "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
        "\n",
        "max_iter = 10000\n",
        "digits_regression_model_2 = LogisticRegression() # Initialize the class\n",
        "digits_regression_model_2.set_values(params_0, alpha, max_iter, 2) # set initial values\n",
        "\n",
        "params = digits_regression_model_2.train(digits_train / 16, digits_label_train, 1000) # Train the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 1000\n",
            "cost: -2.626712096120883\n",
            "--------------------------------------------\n",
            "iteration: 2000\n",
            "cost: -1.385432770398814\n",
            "--------------------------------------------\n",
            "iteration: 3000\n",
            "cost: -0.9246368715530282\n",
            "--------------------------------------------\n",
            "iteration: 4000\n",
            "cost: -0.6812613840724624\n",
            "--------------------------------------------\n",
            "iteration: 5000\n",
            "cost: -0.5299661056524817\n",
            "--------------------------------------------\n",
            "iteration: 6000\n",
            "cost: -0.4264876777128916\n",
            "--------------------------------------------\n",
            "iteration: 7000\n",
            "cost: -0.3511036101217419\n",
            "--------------------------------------------\n",
            "iteration: 8000\n",
            "cost: -0.2936633182478642\n",
            "--------------------------------------------\n",
            "iteration: 9000\n",
            "cost: -0.24839776470809544\n",
            "--------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaJXzkwS-36n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3172632-5230-4148-b5c9-2b6eb61b74c8"
      },
      "source": [
        "digits_accuracy = digits_regression_model_2.test(digits_test / 16.0, digits_label_test)\n",
        "print(f'Accuracy of prediciting a TWO digit in test set: {digits_accuracy}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of prediciting a TWO digit in test set: 0.9972222222222222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU0Kg22D-39-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e71063-2183-4f67-ca92-161b2ab25231"
      },
      "source": [
        "# train a classifier for the EIGHT digit\n",
        "alpha = 1e-2\n",
        "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
        "\n",
        "max_iter = 10000\n",
        "digits_regression_model_8 = LogisticRegression() # Initialize the class\n",
        "digits_regression_model_8.set_values(params_0, alpha, max_iter, 8) # set initial values\n",
        "\n",
        "params = digits_regression_model_8.train(digits_train / 16, digits_label_train, 1000) # Train the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 1000\n",
            "cost: -78.08002168159048\n",
            "--------------------------------------------\n",
            "iteration: 2000\n",
            "cost: -74.2638082454134\n",
            "--------------------------------------------\n",
            "iteration: 3000\n",
            "cost: -72.74890597797143\n",
            "--------------------------------------------\n",
            "iteration: 4000\n",
            "cost: -71.93427300603462\n",
            "--------------------------------------------\n",
            "iteration: 5000\n",
            "cost: -71.419110547916\n",
            "--------------------------------------------\n",
            "iteration: 6000\n",
            "cost: -71.05361853756747\n",
            "--------------------------------------------\n",
            "iteration: 7000\n",
            "cost: -70.77052551851257\n",
            "--------------------------------------------\n",
            "iteration: 8000\n",
            "cost: -70.53625632755923\n",
            "--------------------------------------------\n",
            "iteration: 9000\n",
            "cost: -70.33284385801225\n",
            "--------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDGVEThD9gaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95287c65-7374-45ac-9bbf-dc7e3e5ffa65"
      },
      "source": [
        "digits_accuracy = digits_regression_model_8.test(digits_test / 16.0, digits_label_test)\n",
        "print(f'Accuracy of prediciting a EIGHT digit in test set: {digits_accuracy}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of prediciting a EIGHT digit in test set: 0.9527777777777777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUNg9nbvjYvm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}