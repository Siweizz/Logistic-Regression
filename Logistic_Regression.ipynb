{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6xr7CCyGoLa"
      },
      "source": [
        "### Logistic Regression  \n",
        "Applying logistic regression to the breastcancer dataset for binary classification:\n",
        "\n",
        "**Breast Cancer**:  this dataset is aimed at developing classifiers that can distinguish be-tween malignant and benign tumors in breast cancer.   There are thirty real valued features and 569 instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8XngkyfGoLa"
      },
      "source": [
        "### Task\n",
        "Implement logistic regression using gradient descent. Do ten-fold cross validation and find the best step-size $\\alpha$, using accuracy as the metric. Finally, use the best $\\alpha$ you get and re-train the model on the full training data, and report the accuracy, precision and recall.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlASAFOHGoLb"
      },
      "source": [
        "### A first look at the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ak0bmih8JyG"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "%matplotlib notebook"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvgJ8-RkGoLb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "264df682-e560-473c-e6c6-3dfc4614d462"
      },
      "source": [
        "DATA_DIR='breastcancer.csv'\n",
        "df = pd.read_csv(DATA_DIR, header=None)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0      1       2       3        4   ...      26      27      28       29  30\n",
              "0  17.99  10.38  122.80  1001.0  0.11840  ...  0.7119  0.2654  0.4601  0.11890   1\n",
              "1  20.57  17.77  132.90  1326.0  0.08474  ...  0.2416  0.1860  0.2750  0.08902   1\n",
              "2  19.69  21.25  130.00  1203.0  0.10960  ...  0.4504  0.2430  0.3613  0.08758   1\n",
              "3  11.42  20.38   77.58   386.1  0.14250  ...  0.6869  0.2575  0.6638  0.17300   1\n",
              "4  20.29  14.34  135.10  1297.0  0.10030  ...  0.4000  0.1625  0.2364  0.07678   1\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm3v7SzhGoLb"
      },
      "source": [
        "### Implement the loss function for logistic regression\n",
        "def compute_cost(ip, op, params):\n",
        "    \"\"\"\n",
        "    Cost function in linear regression where the cost is calculated\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    Returns cost\n",
        "    \"\"\"\n",
        "    num_samples = len(op)\n",
        "    cost_sum = 0\n",
        "    c = 0.00001\n",
        "    for x, y in zip(ip, op):\n",
        "        sig_WX = sigmoid(sum(params * x))\n",
        "        #if sig_WX == 1 or sig_WX == 0:\n",
        "        #    cost_sum += int(y == sig_WX)\n",
        "        #else:\n",
        "        cost_sum += y * np.log(sig_WX + c) + (1 - y) * np.log(1 - sig_WX + c)\n",
        "\n",
        "    cost = -cost_sum / num_samples\n",
        "    # End code here\n",
        "    return cost"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql7rdMIPGoLb"
      },
      "source": [
        "#the load_data function is written for you\n",
        "def load_data(data_dir):\n",
        "    df = pd.read_csv(data_dir)\n",
        "    data = np.asarray(df.iloc[:, :-1])\n",
        "    labels = np.asarray(df.iloc[:, -1])\n",
        "    \n",
        "    return data, labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcV0WvA_GoLc"
      },
      "source": [
        "### Implement the sigmoid function \n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As4FE99sGoLc"
      },
      "source": [
        "\n",
        "### Batch gradient descent\n",
        "Algorithm can be given as follows:\n",
        "\n",
        "```for j in 0 -> max_iteration: \n",
        "    for (x_batch,y_batch) in batches: \n",
        "        calculate gradient using (x_batch,y_batch)\n",
        "        update theta\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg5VudO1GoLc"
      },
      "source": [
        "def logistic_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter, batch_size = 1):\n",
        "#     print ('ip.shape ',ip.shape)\n",
        "#     print ('op.shape ',op.shape)\n",
        "#     print ('params.shape ',params.shape)\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using batch gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    max_iter: maximum number of iterations\n",
        "    batch_size: size of mini-batches\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\" \n",
        "    # initialize iteration, number of samples, cost\n",
        "    iteration = 0\n",
        "    num_samples = len(ip)\n",
        "    cost = np.zeros(max_iter)\n",
        "    \n",
        "    # batchify the data into mini-batches\n",
        "    num_batch = int(num_samples / batch_size)\n",
        "    batches = []\n",
        "    for i in range(num_batch - 1):\n",
        "        x_batch = ip[i*batch_size:(i + 1)*batch_size]\n",
        "        y_batch = op[i*batch_size:(i + 1)*batch_size]\n",
        "        batches.append((np.asarray(x_batch), np.asarray(y_batch)))\n",
        "    i += 1\n",
        "    x_batch = ip[i * batch_size:len(ip)]\n",
        "    y_batch = op[i * batch_size:len(op)]\n",
        "    batches.append((np.asarray(x_batch), np.asarray(y_batch)))\n",
        "\n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    while iteration < max_iter:\n",
        "        cost[iteration] = compute_cost(ip, op, params)\n",
        "        \n",
        "        for x_batch,y_batch in batches:\n",
        "            O_minus_y = np.array([sigmoid(sum(params * x_batch[i])) for i in range(len(x_batch))]) - y_batch    \n",
        "            gradient = np.dot(x_batch.T, O_minus_y)\n",
        "            params -= alpha * gradient/len(y_batch)\n",
        "            \n",
        "        iteration += 1\n",
        "    \n",
        "    return params, cost"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWJmHhe-GoLc"
      },
      "source": [
        "def evaluate(x_test, y_test,params):\n",
        "    \"\"\"\n",
        "    Evaluate the model, return the accuracy, precision and recall\n",
        "    x_test: input variables \n",
        "    y_test: labels\n",
        "    params: corresponding parameters\n",
        "    \"\"\" \n",
        "    y_predict = []\n",
        "    true_positive = 0\n",
        "    #false_positive = 0\n",
        "    #false_negative = 0\n",
        "    # sum(params * x_test[i]) > 0 means W_T * x > 0 and sigmoid(W_T * x) > 0.5, \n",
        "    # which means the predict is 1\n",
        "    for i in range(x_test.shape[0]):\n",
        "        y_predict.append( int(sum(params * x_test[i]) > 0))\n",
        "        if int(sum(params * x_test[i]) > 0) == 1 and y_test[i] == 1:\n",
        "          true_positive += 1\n",
        "        #if int(sum(params * x_test[i]) > 0) == 1 and y_test[i] == 0:\n",
        "        #  false_positive += 1\n",
        "        #if int(sum(params * x_test[i]) > 0) == 0 and y_test[i] == 1:\n",
        "        #  false_negative += 1\n",
        "\n",
        "    acc = np.sum(y_predict == y_test) / y_test.shape[0]\n",
        "\n",
        "    #precision = true_positive / (true_positive + false_positive)\n",
        "    #recall = true_positive / (true_positive + false_negative)\n",
        "    precision = true_positive / sum(y_predict)\n",
        "    recall = true_positive / sum(y_test)\n",
        "    return acc, precision, recall\n",
        "            \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKSqdRToGoLc"
      },
      "source": [
        "def cross_validation(x_train_full, y_train_full, k, alpha, max_iter, batch_size):\n",
        "    \"\"\"\n",
        "    For a given set of hyper-parameters, do k-fold cross_validation, return the average accuracy.\n",
        "    x_train_full: the full training data\n",
        "    y_test: the full training labels\n",
        "    k: number of folds to divide the whole traiing set\n",
        "    alpha: step-size for Gradient Descent\n",
        "    max_iter: maximum number of iterations\n",
        "    batch_size: size of mini-batches\n",
        "    Return the average accuracy\n",
        "    \"\"\"\n",
        "    #split the data into k-folds\n",
        "    all_accs = []\n",
        "    num_samples = y_train_full.shape[0]\n",
        "    fold_size = int(y_train_full.shape[0] / k)\n",
        "    # Shuffle the data\n",
        "    shuffler = np.random.permutation(len(y_train_full))\n",
        "    x_train_shuffled = x_train_full[shuffler]\n",
        "    y_train_shuffled = y_train_full[shuffler]\n",
        "    # Generate fold index\n",
        "    fold_idx = {}\n",
        "    for i in range(k - 1):\n",
        "        fold_idx[i] = np.array(range(i * fold_size, i * fold_size + fold_size))\n",
        "    i += 1\n",
        "    fold_idx[i] = np.array(range(i * fold_size, x_train_shuffled.shape[0]))\n",
        "\n",
        "    for idx in range(k):\n",
        "        ###during each iteration, we hold out one hold for evaluation, and use the rest as training data\n",
        "        train_index = np.delete(np.array(range(x_train_shuffled.shape[0])), fold_idx[idx])\n",
        "        x_train = x_train_shuffled[train_index]\n",
        "        y_train = y_train_shuffled[train_index]\n",
        "        x_eval = x_train_shuffled[fold_idx[idx]]\n",
        "        y_eval = y_train_shuffled[fold_idx[idx]]\n",
        "        # train the data\n",
        "        #initial_params_guess = np.ones(x_train_full.shape[1]) / 1000\n",
        "        initial_params_guess = np.zeros(x_train_full.shape[1])\n",
        "        params, cost = logistic_regression_using_batch_gradient_descent(x_train, y_train, initial_params_guess,\n",
        "                                                                  alpha, max_iter, batch_size)\n",
        "        #After figuring out the training and eval data, train you model and evaluate it to get the accuracy.\n",
        "        acc, precision, recall = evaluate(x_eval, y_eval, params)\n",
        "        all_accs.append(acc)\n",
        "    average_acc = sum(all_accs)/k\n",
        "    return average_acc"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83_NpaeOGoLd"
      },
      "source": [
        "### Cross-validation to find the best alpha"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pW1pwzhGoLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d419a9-ad22-4114-a557-dd117b9b3a23"
      },
      "source": [
        "#Set your DATA_DIR\n",
        "DATA_DIR='breastcancer.csv'\n",
        "# Do not change the code below\n",
        "# Train the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "#reserve the test data, do not use them for cross-validation!\n",
        "data, labels = load_data(DATA_DIR)\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.20)\n",
        "x_train = np.append(x_train, np.ones((x_train.shape[0],1)), axis=1)\n",
        "x_test = np.append(x_test, np.ones((x_test.shape[0],1)), axis=1)\n",
        "\n",
        "## For each possible value of alpha, do cross-validation\n",
        "##compare the average accuracies and choose the best alpha\n",
        "alphas = [5e-4,1e-4,5e-3,1e-3,5e-2]\n",
        "max_iter = 200\n",
        "batch_size =16\n",
        "cv_results =[]\n",
        "for alpha in alphas:\n",
        "    average_acc = cross_validation(x_train, y_train, 10, alpha, max_iter, batch_size)\n",
        "    cv_results.append([average_acc,alpha])\n",
        "cv_results.sort()\n",
        "print (cv_results)\n",
        "best_alpha = cv_results[-1][1]\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.7853514739229025, 0.0001], [0.8672108843537416, 0.005], [0.8831292517006801, 0.05], [0.9036734693877552, 0.001], [0.9181405895691611, 0.0005]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v1HhMzZGoLd"
      },
      "source": [
        "### Re-train on the whole training set with the best alpha, report results on test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQySghyIGoLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8fbb97-9c6e-4bcc-f6d7-d5c7f4b37491"
      },
      "source": [
        "# Do not change the code in this cell\n",
        "# Re-train the model using the best alpha you picked\n",
        "# Report the final results\n",
        "\n",
        "params = np.zeros(x_train.shape[1])\n",
        "params, costs =\\\n",
        "    logistic_regression_using_batch_gradient_descent(x_train, y_train, params, best_alpha, max_iter, batch_size=10)\n",
        "final_acc,final_precision,final_recall = evaluate(x_test, y_test,params)\n",
        "print ('Final results : Accuracy:{:.2f}, Precision:{:.2f},Recal:{:.2f}'.format(final_acc,final_precision,final_recall))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final results : Accuracy:0.91, Precision:0.91,Recal:0.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJWoRmgUGoLd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b3eed354-7799-490c-e58b-522b991b6e64"
      },
      "source": [
        "plt.plot(costs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa7ab6dfe50>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhdd33n8ff3nrtpt2zLS+w4dvYdbEygCVsTShZSUtoyTVs6pctk6LQdeFrKQJlpaaedp5S2tHSBmkJX1kBSeKAUUiAkoSREduw4i7M5jndbtrVLdzvnN3+ccxfJV9aV7SsdX31ez+NH8tX11ddH8sc/fX/LMeccIiISX4mFLkBERE5NQS0iEnMKahGRmFNQi4jEnIJaRCTmks140eXLl7v169c346VFRFrS1q1bjznn+up9rClBvX79evr7+5vx0iIiLcnMXprpY2p9iIjEnIJaRCTmFNQiIjGnoBYRiTkFtYhIzCmoRURiTkEtIhJzsQ3q7z1/jD3Hxhe6DBGRBRfboH7P3Tv4xIO7F7oMEZEFF9ugLpQCCqVgocsQEVlwsQ3qwDn8QHefERGJcVCDr9uEiYjEOag1ohYRgTgHdaCgFhGBBoLazC4zs+01v0bM7N3NLixwKKhFRGjgPGrn3DPAywHMzAMOAPc2uS61PkREInNtfdwEvOCcm/GA67MlcE6TiSIizD2o7wQ+W+8DZnaXmfWbWf/AwMAZF6bWh4hIqOGgNrM08Bbg7nofd85tcc5tds5t7uure9uvOVHrQ0QkNJcR9a3ANufckWYVU+acw2lELSICzC2of5oZ2h5nWzmfFdQiIg0GtZl1AD8C3NPcckJBNImoyUQRkQaW5wE458aBZU2upaIc1IFG1CIi8dyZGESH5pUU1CIiMQ3qcutDQS0ioqAWEYm7eAZ11PrQZKKISFyDWpOJIiIVsQ5qTSaKiMQ2qKO3CmoRkbgGtUbUIiJlsQ7qQJOJIiJxDerwrZbniYjENagDtT5ERMriGdRaniciUhHToA7fakQtIhLToC73pjWZKCIS06B2OutDRKQilkFd2fDiqqEtIrJYxTSoq+GsUbWILHaxDOracNaEoogsdrEM6tpuhyYURWSxi2VQ14azRtQistjFMqhrbxigTS8istg1FNRmtsTMvmhmu8zsaTP7oWYW5TSZKCJSkWzweX8B/Ltz7ifNLA20N7EmarNZQS0ii92sQW1mPcDrgHcAOOcKQKGZRdWGs+6bKCKLXSOtjw3AAPD3ZvaYmf2dmXVMf5KZ3WVm/WbWPzAwcEZFTZlM9BXUIrK4NRLUSWAT8DHn3EZgHHjf9Cc557Y45zY75zb39fWdUVFaniciUtVIUO8H9jvnHol+/0XC4G4a7UwUEamaNaidc4eBfWZ2WfTQTcBTzSxqSo9aQS0ii1yjqz5+Hfh0tOJjN/ALzStpautDk4kistg1FNTOue3A5ibXUqHJRBGRqnjuTKxpd2gyUUQWu1gGtTa8iIhUxTKotYVcRKQqlkGtEbWISFUsg9rXiFpEpCKWQT2l9aHJRBFZ5GIZ1NqZKCJSFcug9oPa9xXUIrK4xTKoNaIWEamKZVBreZ6ISFUsg3pK60OTiSKyyMUyqNX6EBGpimVQq/UhIlIVy6DWzkQRkapYBrVuHCAiUhXLoA60M1FEpCKWQe3U+hARqYhlUOtQJhGRqlgGtZbniYhUxTKo1foQEamKZVBPWfWhyUQRWeQaugu5me0BRgEfKDnnmnpH8imtD92FXEQWuYaCOvLDzrljTaukxpQNLxpRi8giF8vWh3OOhIXvB+pRi8gi12hQO+CbZrbVzO6q9wQzu8vM+s2sf2Bg4IyK8gOHlzCSCaOkoBaRRa7RoH6Nc24TcCvwq2b2uulPcM5tcc5tds5t7uvrO6OiAgdmRiJhan2IyKLXUFA75w5Eb48C9wLXNbOocuvDM9NkoogserMGtZl1mFlX+X3gTcATzSzKDxyeha0PjahFZLFrZNXHSuBeMys//zPOuX9vZlGBg0TU+tBkoogsdrMGtXNuN/CyeailInAOMzSZKCJCnJfnJaIRtVofIrLIxTKofVftUZc0mSgii1wsg7qyPM80mSgiEsugrizP02SiiEg8g1o7E0VEqmIZ1FOW56n1ISKLXEyDumZ5niYTRWSRi2dQR62PhNUfUZf8gM/+YC8lP1iA6kRE5lc8gzpqfXgJq3srrkf3DPL+e3by4PPzcjy2iMiCimlQh60Pb4bJxFzRB2D/iYn5Lk1EZN7FMqhdzYi6XusjXwqDet/g5HyXJiIy72IZ1OXT87wZJhPzpbA3vU8jahFZBGIZ1JXWxwyTiYUoqPdrRC0ii0BMgzpsfSS9+pOJhWi1x75BjahFpPXFNKiry/PqBXW+GAb10ESR0VxxvssTEZlXsQ3q8lkf9Q5lKtSsn1b7Q0RaXUyDOjw9b6bJxHKPGjShKCKtL55BHVRvbnuqyUTQEj0RaX2N3DNx3pV71J5Xf8NLvuSTTSXwzNivCUURaXGxDWozC0fU9VZ9lAIySY9V3Vn2ndCIWkRaWzxbH45ZJxPTyQQre7IMjOYWoEIRkfnTcFCbmWdmj5nZV5tZEFRPz/MShj/DzsS0l6A95ZEr6gQ9EWltcxlRvwt4ulmF1AqX54Wtj3oj6nwpIJNKkE0lmIwOaBIRaVUNBbWZrQXeDPxdc8sJVZbnzbQzMRpRt6W9ykl6IiKtqtER9Z8D7wVm7DOY2V1m1m9m/QMDA2dUVGXDyww7E8PJxASZpKcRtYi0vFmD2sxuB44657ae6nnOuS3Ouc3Ouc19fX1nVFTgqqfn1d1CXvJJJ8MRdV49ahFpcY2MqG8A3mJme4DPATea2b80s6ggqO5MnHlE7ZFNehT8oO5zRERaxaxB7Zx7v3NurXNuPXAn8G3n3NubWVQjZ32EI+qwfPWpRaSVxXQddc3yvFNNJqY8APWpRaSlzWlnonPufuD+plRSo3Jz25mOOY2W52XKQV1QUItI64rtiLp8c9vAgZvW/pg+oi7fQ1FEpBXFM6hrdiYCJ42qC6WwR52tjKi18kNEWlc8g7rmLuTASROK5aAuj6hzGlGLSAuLaVBXWx9w8og6X16elwrLV49aRFpZPIM6qJ71AVOD2jlXWZ5Xbn1oeZ6ItLJ4BrWjsjMRwg0wZeX7JWZqe9QKahFpYTENakciUW19lGqSOh/dhqt8KBOgbeQi0tJiGtThFvJEncnE8v0SM6kE2WTUo9aIWkRaWEyDOjyUKVlnMrFQZ0StoBaRVhbboC4fcwozBHUyQTapyUQRaX3xDOogvLltps7yu3KPOpP0SCSMdFJ3eRGR1hbPoI42vKztbQNg/2D1TuO1I2qAtpTOpBaR1hbToHZ4CTh/aTsALx0fr3ys4Iej53JQZ1MJbXgRkZYW26BOmNHXmaEt5bH3RHVEXbs8D8IRtbaQi0gri2dQR3d4MTPWLW1n74mJysfyNcvzALIpTyNqEWlp8QzqqPUBYftjX01QF6aNqLMpj1xJPWoRaV2xDepEtDSvPKIun0ld2fBS06POaUQtIi0spkEdtj4A1i1tY7Loc2ysANT0qJPqUYvI4hC7oA6izS3lzS7rloUrP8p96kLNOmpQj1pEWl/8gjpqcUS7x1kXLdHbVwnqqcvz2lKeNryISEuLYVCHb8sHMq3tnTai9qe2PjIpj5w2vIhIC5s1qM0sa2Y/MLMdZvakmf1eMwsqj6ijzgfZlMeq7mwlqMu7EKeso9aIWkRaWLKB5+SBG51zY2aWAh4ys6875x5uRkHloC73qAF62lKM5opAOKI2g5QXfjybSiioRaSlzTqidqGx6Lep6Jc7xR85I5XWR01Qp5OJyiRioRSQ9hKVVSFtKY9S4Cj6an+ISGtqqEdtZp6ZbQeOAvc55x6p85y7zKzfzPoHBgZOu6DykaY1OR0GdRTE+egO5GXlM6k1qhaRVtVQUDvnfOfcy4G1wHVmdnWd52xxzm12zm3u6+s77YLKG1vKt+GCcHNLeURdvgN55WO6b6KItLg5rfpwzg0B3wFuaU45jbU+MrUj6pTumygira2RVR99ZrYker8N+BFgV7MKmr6OGsIVHuUdiQV/ausjW765gEbUItKiGln1sRr4RzPzCIP9C865rzaroKDSo542ovbLI2q/sjQPqiNq7U4UkVY1a1A75x4HNs5DLUC19VHbo04nE5XWRr4UVI44hXCdNcCTB0dIJxNcsbqb7z47wPNHx/il12yYr7JFRJqmkRH1vKrX+shMGVEHU0bU5aD+7Xt30pVJ8m/vei2/dfcOAucU1CLSEmK3hdyv1/rwpq2jrulRr1vaznk9WX7yFWvJlXx+8uP/ydHRfKWnLSJyrovdiNqVWx8zrPrIlwK6stWy+7oy/Of7bwJgaUeaLQ/sBqqn7ImInOtiF9SV1kfNWL92MjFX9Cvtjul+/caLGc+XGJos8m87D+GcmzIyFxE5F8Wu9VHtUde2Pjz8wOEHLtrwUr/srmyKP3zrNVy+sgvnoBQ0bae7iMi8iW1QT1+eB2E741Qj6rJU9Hyd/yEirSCGQR2+nd6jhsaDurwqRH1qEWkFMQzqOjsTo6DO+/4pWx/Tn6+gFpFWELugrrc8rxzM+WIQbXiZZURdfr6CWkRaQOyC2tXZmVgO6tFcCaie7zGT8vML6lGLSAuIXVDPdCgTwEh0l5dsUj1qEVk8YhfU5dbH9GNOoTqizswyolaPWkRaSeyCevpdyKEavCOTDY6o1foQkRYSu6B2jbQ+GlyeV9SIWkRaQOyCeqY7vEBN62OW5XmpynI+BbWInPtiF9Qz3dwWalof2vAiIotI7IK6cnPbOuuoq62PBpfnKahFpAXELqjrTiZ64Qi62vpocDJRQS0iLSB2Qe2fYgv58GRjI2qt+hCRVhK7oK57zOlJrQ/1qEVk8YhdULtTBHWjqz7SOuZURFrIrEFtZueb2XfM7Ckze9LM3tXMgsrZmqg3mRi1PnQok4gsJo3ciqsE/KZzbpuZdQFbzew+59xTzSioeuOAmiIThhmMNHgoUyqh1oeItI5ZR9TOuUPOuW3R+6PA08CaZhVUWZ5XM5toZqS9BH7gMKv2oGeSSBgpzzSZKCItYU49ajNbD2wEHqnzsbvMrN/M+gcGBk67oHo7E6HazsgkEw3dsDbtJU4aUX9lx0Fu/NP7KSnAReQc0nBQm1kn8CXg3c65kekfd85tcc5tds5t7uvrO+2CqqfnTX283KeebcVHWToZBnW+5HN8LA/ArkMj7B4Y59Bw7rTrExGZbw0FtZmlCEP60865e5pZUGV53rSkLrc7Zjs5r/L8KKg/8cBubv/LhwCYKPgA7Dk+frbKFRFpukZWfRjwSeBp59yfNbsgN1vrY5aJxNrnF/yAA0M5Dg3nCALHRCGcjHzp+MTZK1hEpMkaSb0bgJ8DbjSz7dGv25pV0Eytj3JQNzyi9sKgHs+H4TxZ9BmPRtR7TyioReTcMevyPOfcQ8Dss3dnSb2diVAT1A2PqD0KpYAgCv7xfImJKLT3HFPrQ0TOHY2so55Xrs6hTFDtUc92IFPl+VGPOlcMR9HjBb/So9aIWkTOJbHbQl7v5rZwGj1qzyiUAsajvvR4vlQJ6peOT1TWa4uIxF3sgtqfsfURjqTntDzPDxjPRyPqfKkymThZ9BkYzZ+tkkVEmip2QT3jhhdvjuuoow0vY1FfeiJqfazoygDwktofInKOiF1Q17u5LVRbHrOdnFdW7lGXV32MF0qM50tcsbob0BI9ETl3xC6oS/7JZ30AZLy5r/rIlaoTiBN5n8miz6UrO0kYvKRNLyJyjohdUJdHwO3pqQtSTmcd9dBEsfL7ockCRd/R05ZiaUeaY2OFs1SxiEhzxS6ox/IlMslEJZjLTmdnYvmOMEBl8rAtnWRJe5qhCQW1iJwbYhfUI7kSXdnUSY/P9ayPTDJB7Qq8clB3pD1621NTRtsiInEWu6Aey5foyp68Dyc9x9PzUt7UHvdAdIJeeyZJT1uaQY2oReQcEb+gzhXpzMwc1HNpfdQqj6jbUxpRi8i5JX5BPduIuuHJxKnPqwR1xqO3QyNqETl3xC6oR3Ol+iNq7/RH1N3ZJIPRCLo9nWRJe4p8zTkgIiJxFs+grjOizlRuxdX4FvKyFd3ZyvsdaY8lbWkAjapF5JwQu6Aey5forrfqY87HnNYEdbRtHMLJxN728PUHx9WnFpH4i1VQO+cYy8/Q+pjjqo9MzZ3K+2qDOuWxpD0cUWsttYicC2IV1JNFHz9wM7Q+5n56HoStjtrgDycToxG1Vn6IyDkgVkE9lgu3j9cbUV+zpofrL1rGhX0dDb1WKhpRd2SSdESv5yWMtJeo9KiHJjWiFpH4i9UdXkaioK63PO/8pe185r+9uuHXKo+oOzNJ2tPhKLw97WFmLIl61FpLLSLngniNqPMzB/VclYO6PePRER3wVH6bTXm0pTwGxzWiFpH4i1dQV1ofJ6/6mKvyuuuOdLX1UR5ZAyxpT53Uoy6UAkp+cMafW0TkbJo1qM3sU2Z21MyeaHYxY/kwOM/miLozk6QjE7U+MrVBnWZ4Wo/6p7Z8nw/cW/1r7j0+we1/+SD/9P09+IHusSgiC6OREfU/ALc0uQ6g2qOuN5k4V+UNMh2ZZOVs6/ZU9XV7p42oj4/leWzvEF/beYh8KdyxePfWfTxxYITf+fKT/NbdO864JhGR0zFrUDvnHgBOzEMtldbH2RxRd2SSdKRPHlH3tk897+PRPeFfcSxf4uHdJ3DO8bXHD3H9Rcu485Xn89Wdh5gsaMu5iMy/s9ajNrO7zKzfzPoHBgZO6zXKk4lnY0Rd7lF3ZjzaM1MnEwF6pp2g98iLJ8imErSnPe576jC7Do+y+9g4b752Nbdes5pCKeCRF49P+Ry5os+x6PjU4cki+3TDXBFpgrO2PM85twXYArB58+bTauiO5oq0pTyS3pn//1E7ou6MRtJt6doRdYqhiQJB4EgkjEd2n2DTul562lLc99QRCqWAhMHNV62iM5Mkk0zwwLPHeMNlKyqv8Z67d/D1Jw5z81Ur+d7zx/EDxyO/fVNl8rJsJFfkg195ktdf2scdL19zxn83qW94okhP+6knop1z2LQ73IvEXbxWfeTrH8h0Oqauoy6PqKtBvbwzQ+DgZb//Td77xR08fXiEV21Yxi1Xr+LISJ4v9O/ntZf0sbwzQzblcd2GpXz32aOVP//kwWG++vghrl7Tw7eePsra3jbG8iW+vevolDqOjOR428e+zz3bDvCn33y2cpf1IHDsOVb/BruBJi7n7LvPDrDpD+7jC/37TvpY/54TjOdLHB3N8foP38/fPbj7lK+16/AIB4Ymm1WqyJzFasPLaK7+WdSnY1lHmv/95iu4/drzaEuVR9TV1/7xTWvxA8dTh0b44tb9OAfXbVjKdRuWsrwzQ1va4/JVXZXnv/7SPv7ga0/zlr96iIQZjvD41H/6xevoaUvhB45X/b9v8W87D/HGK1bywsAYl6zs5K5/3sq+wQl+5lXr+Mwje+l/aZArV3fz7s9v576njvA3P7uJ265ZXfk8j+w+zi/8w6O8+ZrVbLqgl3997AAXr+ikryvDw7uP887XX1QZ1W/bO8jd/ft42dol3Hbt6pMOs3r60AgvDIyR8hK84bK+hk8ebEQzR6Zzfe18yed3v/wEfuD4o6/v4uarVtHTFl6LTz30Ir//1afYuG4Jve1p9p6Y4I+/8Qw3Xr6CC/s6T3qt7fuG+Km//T5d2RT3/Mr1OByTRZ9LVnThJerX9OieEwxPFLnpihUN1T2WLzE0UWBtb3vDf8ezyQ8cB4cm6evKNHwkA0DRDxicKNCeTtKe8kjMcD1KfsDx8QKBc3RlUxgQOEfRDz9vvuSzuqeNFV2Zyk/PJT9gouiTK/p0Z1MkE8ZLJyZIJozubIqxfAk/cATOMThRIHDh7uPxfIkVXRkuXhF+LZ2jUlfJD9ixf5j1y9pZ1pmpW6sfOB58boDnj45hZrxt81q6MklyxWDKT+C7B8Y4NlZgaUeaQilgZXeGZZ0ZXjo+zpGRPK+4oJdnDo/y3NHRpvzUbM6devRmZp8F3gAsB44Av+uc++Sp/szmzZtdf3//nIt5x9//gMHxAl/+tdfM+c+eSskP2Ph/7+M9b7qMn79+/Ukf37Z3kPufGeB/3njxjG2XvccnuPUvHuDilV2M5orsHhjnPW+6lF+78ZLKc37ny0/w+Uf3cdV53WzbO8TK7gxHRvJ8/O2v4LWXLOeVf/gf3HDxcvYPTvLM4RFWdmfxA8d//ObrSZhRLAW8+aMPki8FDE8WKQWOC5d3cGg4R64UfgP7gePe/3E9SS/BW//me4xMFgkcXL6qi8/d9Wp62lJs2zvIpx7aw9d2HqrUtronyy1Xr2J5Z4abr1rJ3hMT/P339tDXmSFwjp0HhnnrxjW88/UXMVn0+fA3nuHRPYO8cn0vBwYnOTZe4O2vWsfNV6/iqYMj/OYXdrBuaTu/cMN6tjywmz3HJ8gkE2RSCS5b2cWPXLmS7fuGeGzvEHtPTJAv+eH2/fY0JT8gk/K48rxurj6vh7F8kc8/uo+2tMeyjgy7Do+Q8hLhYVoOfOfwA4eXMDwzEjVv0174n+bj+4d5362X86F/38UNFy3nqjXdDIzkueexA7z8/CXsPDCMHzj+++su5DM/2EtXJslYvkRvR5pr1y6hPeWR9IxvPHmEbCrBWL5EyXeVeZP2tMfqniwpL8HwZJFsyqMrm6ToO54+NALAdeuX4iWMF4+N09OWYkl7ir6uDJes6GLP8XF27BtiJFfk2Fg4if2KC3pZ1pHmyYMjlILG1u+3p5Os7W1jouAzWfBZ1pkm7SXwnaP8g1g5Ps3CcN0/OEm+GJBNJcimPA4MTTKaK2EW/sRpNa+9sjvDiYkCE3mfnvYUE3mfiUKJjkySgdE8peiTmIU38XCEn9dFnz9wjlkipcJLGO1pj1zRp+hP/UMpz0567FT6ujKM5orkSwGdmSQblndwdCTP4ZEcZuEJmhN5n5U9WVZ1ZxnLl0gmjINDkxwczlVep7c9RUcmyf7BSa5c3c15S9o4MDRZ+RqXpb0EN1y8jAefO0YpcHRlkozmS/S0pXj0A2886Q5TjTCzrc65zXU/NltQn47TDeqf+Nh/kk0l+PQvN75VvFFHR3IsaU+f1gUsK4/0/MDx+P4hrl27ZMoo6+Hdx7lzy8MkDN5x/Qa+vesIb924lne9MQzz3/jCdu7ZdoDOTJK/+pmNLOvIcMdfP0TKS5Avhf9QkwnjS79yPZ3ZJIPjBV5xQS/5UsBkwSdX8vnRv/weo7kiyYSRTib411+9gRcGxnjnv2xjaXuaUhBwbKxAe9rjl197IW++ZjUHhyf52Hde4MmDw4zXrFxZs6SNgh/gnOOCZR1sfWmQ3uimCpNFn03retl5YJhV3VmyqQTPHhmr/Nnzl7YxNF5kNBrR3HTFCgolR67o8/3dxzkxXqAt5bF5fS/rl3VU/kEOTRZJeQnGciWeODjM/sFJzOCmy1eQSXocG8tzxepunHMcGytgFl6ThBmBc5SiUZUfOPwACn7AwGie69b38nt3XM1H7nuWTzy4m5LvWNKe4roNS/mTt72M7+w6yiMvnuD/3H4lX95+gL/6zvNsvqCXoYkiuw6PhpudgoDuthR/+/ZXMJIr8aGv7+INl/exsivLzgPDHB7O4TtHT1t4jUZzRXJFn5uvWkU6meAj9z1LX1eWK1d3M5orMjRZ5NDwJPtOTLK0I80r1/eytCPNmiVtJL0EX3h0H6XAsXHdkspPfbMZzZXYPzhBezpJW9rj+HgBPwhImE0dzTuHIwzDNUvaousffl2Xd2a4Zk0PA6P5KSufxvIljozkWNqRpiOTZHiiSEcm3MU7lvdZ0Z1hdU+WyYLPeL7EZNGvfF4zwv88DbxEgqWdaTwzRnNFzCBhhpcwVvdkySQ9Dg3nODQc/ofRlvZoT3m0pT0y0Y7hiYJfGSWPTIa350tG90Ht7UiTSiQo+D7t6SQvDIyxdc8gvR1p2tMew5NFXjw2Tibpcfu1q9l7YoJ9JyboyCQ5MDTJsbE8nZkkgXO0pTx+YtNafuiiZewfnOSj33oOgEtXdrH1pUGGJov0tCV54xUruWRlF4PjBTLJBA89f4yv7TzErVev5tUXLuWBZ49x7doe3vKy8+jtSM8hVarOmaC++SMPsH55O3/7c3VrjT0/cPyvLz3OGy7r4/Zrzzvp488cHuXD39jFe2+5nEtXhm2Vf3n4JZ48OMy6pR2M5oq8csNSfrhmwrLea3z2B3uZLPi8/dUXcM3aHgDuf+Yon3zoRVZ1Z3nVhWGvvd7qmWNjef71sQNkUh4/tfn8Kf9xfX3nIe5/ZoB0MsGPb1rDxnW9lZGsc47vv3CcnQeGKQWOd1y/nuHJIt96+gg/tnHNlDvH50s+Tx0c4fJV3VN+fKxnaKJAoRRMublDq5kolMgmZ24ViMA5FNQ3/NG3+aGLlvEnb3vZWa9JRCTOThXUsVr1MTLDHchFRBazWAX1TZev4NroR3kREQnFavj653duXOgSRERiJ1YjahEROZmCWkQk5hTUIiIxp6AWEYk5BbWISMwpqEVEYk5BLSIScwpqEZGYa8pZH2Y2ALx0mn98OXDsLJZztqiuuYtrbaprblTX3J1ObRc45/rqfaApQX0mzKx/poNJFpLqmru41qa65kZ1zd3Zrk2tDxGRmFNQi4jEXByDestCFzAD1TV3ca1Ndc2N6pq7s1pb7HrUIiIyVRxH1CIiUkNBLSISc7EJajO7xcyeMbPnzex9C1jH+Wb2HTN7ysyeNLN3RY9/0MwOmNn26NdtC1TfHjPbGdXQHz221MzuM7Pnore981zTZTXXZbuZjZjZuxfimpnZp8zsqJk9UfNY3etjoY9G33OPm9mmBajtw2a2K/r895rZkujx9WY2WXPtPj7Pdc34tTOz90fX7Bkzu3me6/p8TU17zGx79Ph8Xq+ZMqJ532fOuQX/BXjAC8CFQBrYAVy5QLWsBjZF73cBzwJXAh8E3hODa7UHWD7tsT8G3he9/z7gQwv8tTwMXLAQ1wx4HbAJeGK263suVIEAAAOASURBVAPcBnwdMODVwCMLUNubgGT0/odqaltf+7wFqKvu1y76t7ADyAAbon+33nzVNe3jfwr8zgJcr5kyomnfZ3EZUV8HPO+c2+2cKwCfA+5YiEKcc4ecc9ui90eBp4E1C1HLHNwB/GP0/j8CP7aAtdwEvOCcO92dqWfEOfcAcGLawzNdnzuAf3Khh4ElZrZ6Pmtzzn3TOVeKfvswsLZZn38udZ3CHcDnnHN559yLwPOE/37ntS4zM+C/AJ9txuc+lVNkRNO+z+IS1GuAfTW/308MwtHM1gMbgUeih34t+tHlU/PdXqjhgG+a2VYzuyt6bKVz7lD0/mFg5cKUBsCdTP3HE4drNtP1idv33S8SjrzKNpjZY2b2XTN77QLUU+9rF5dr9lrgiHPuuZrH5v16TcuIpn2fxSWoY8fMOoEvAe92zo0AHwMuAl4OHCL8sWshvMY5twm4FfhVM3td7Qdd+LPWgqy5NLM08Bbg7uihuFyzioW8PqdiZh8ASsCno4cOAeuccxuB3wA+Y2bd81hS7L520/w0UwcE83696mRExdn+PotLUB8Azq/5/drosQVhZinCL8CnnXP3ADjnjjjnfOdcAHyCJv24Nxvn3IHo7VHg3qiOI+UfpaK3RxeiNsL/PLY5545ENcbimjHz9YnF952ZvQO4HfjZ6B84UWvhePT+VsJe8KXzVdMpvnYLfs3MLAn8OPD58mPzfb3qZQRN/D6LS1A/ClxiZhuiUdmdwFcWopCo9/VJ4Gnn3J/VPF7bU3or8MT0PzsPtXWYWVf5fcKJqCcIr9XPR0/7eeDL811bZMooJw7XLDLT9fkK8F+jWflXA8M1P7rOCzO7BXgv8Bbn3ETN431m5kXvXwhcAuyex7pm+tp9BbjTzDJmtiGq6wfzVVfkjcAu59z+8gPzeb1mygia+X02H7OkDc6k3kY4e/oC8IEFrOM1hD+yPA5sj37dBvwzsDN6/CvA6gWo7ULCGfcdwJPl6wQsA74FPAf8B7B0AWrrAI4DPTWPzfs1I/yP4hBQJOwF/tJM14dwFv6vo++5ncDmBajtecL+Zfl77ePRc38i+hpvB7YBPzrPdc34tQM+EF2zZ4Bb57Ou6PF/AN457bnzeb1myoimfZ9pC7mISMzFpfUhIiIzUFCLiMScglpEJOYU1CIiMaegFhGJOQW1iEjMKahFRGLu/wO6+PINnsq/CgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}